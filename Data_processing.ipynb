{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dLjXcjaes2O5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random   \n",
        "import pandas as pd\n",
        "# from underthesea import pos_tag\n",
        "list1 = \"qwertyuiopasdfghjklzxcvbnmư\"\n",
        "list2 = \"`1234567890-=qwertyuiop[]\\\\asdfghjkl;'zxcvbnm,./\"\n",
        "dict1 = {'à':'aw', 'á':'as', 'ả':'ar', 'ã':'ax', 'ạ':'aj', 'ă':'aw', 'ằ':'afw', 'ắ':'asw', 'ẳ':'arw', 'ẵ':'axw', 'ặ':'ajw', 'â':'aa', 'ầ':'aaf', 'ấ':'aas', 'ẩ':'aar', 'ẫ':'aax', 'ậ':'aaj', 'đ':'dd', 'è':'ef', 'é':'es', 'ẻ':'er', 'ẽ':'ex', 'ẹ':'ej', 'ê':'ee', 'ề':'eef', 'ế':'ees', 'ể':'eer', 'ễ':'eex', 'ệ':'eej', 'ì':'if', 'í':'is', 'ỉ':'ir', 'ĩ':'ix', 'ị':'ij', 'ò':'of', 'ó':'os', 'ỏ':'or', 'õ':'ox', 'ọ':'oj', 'ô':'oo', 'ồ':'oof', 'ố':'oos', 'ổ':'oor', 'ỗ':'oox', 'ộ':'ooj', 'ơ':'ow', 'ờ':'owf', 'ớ':'ows', 'ở':'owr', 'ỡ':'owx', 'ợ':'owj', 'ù':'uf', 'ú':'us', 'ủ':'ur', 'ũ':'ux', 'ụ':'uj', 'ư':'uw', 'ừ':'uwf', 'ứ':'uws', 'ử':'uwr', 'ữ':'uwx', 'ự':'uwj', 'ỳ':'yf', 'ý':'ys', 'ỷ':'yr', 'ỹ':'yx', 'ỵ':'yj'}\n",
        "def create_Error(sent, pivot):\n",
        "    # kiểm tra pos tag của word trong câu, không tạo lỗi trên các word có tag 'Np\"\n",
        "#     sent_postag = pos_tag(sent)\n",
        "    tokens = [x for x in sent.split()]\n",
        "    out = []\n",
        "    count = 0\n",
        "    for i in range(len(sent.split())):\n",
        "        token = sent.split()[i]\n",
        "        # nếu là dấu hoặc các từ đặc biệt thì giữ nguyên, không tạo lỗi\n",
        "        if not token.isalpha():\n",
        "            out.append(0)\n",
        "            continue\n",
        "#         # không tạo lỗi trên các word có tag 'Np\"\n",
        "#         if sent_postag[i][1] == 'Np':\n",
        "#             out.append(0)\n",
        "#             continue\n",
        "        prob = random.random()\n",
        "        # nếu word trong 1 câu có xác suất nhỏ hơn pivot thì tiến hành tạo lỗi trên nó\n",
        "        # make sure that number of error not large than 1/2 total number of tokens\n",
        "        if prob < pivot and count <= len(tokens) // 2:\n",
        "            count = count + 1\n",
        "            prob = prob/pivot\n",
        "                # đổi chỗ vị trí 2 char\n",
        "            if prob < 0.2:\n",
        "                modified_word = list(token)\n",
        "                word_length = len(modified_word)\n",
        "                if word_length - 2 <= 0:\n",
        "                  out.append(0)\n",
        "                  continue\n",
        "                modified_index = random.randint(0, word_length - 2)\n",
        "                modified_word[modified_index], modified_word[modified_index + 1] = modified_word[modified_index + 1], modified_word[modified_index]\n",
        "                tokens[i] = ''.join(modified_word)\n",
        "                out.append(1)\n",
        "                # thêm 1 char vào word\n",
        "            elif prob < 0.4:\n",
        "                modified_word = list(token)\n",
        "                word_length = len(modified_word)\n",
        "                modified_index = random.randint(0, word_length - 1)\n",
        "                y = random.randrange(len(list1))\n",
        "                tokens[i] = ''.join(modified_word[:modified_index]) + list1[y] + ''.join(modified_word[modified_index:])\n",
        "                out.append(1)\n",
        "                # xóa 1 char khỏi word\n",
        "            elif prob < 0.6:\n",
        "                modified_word = list(token)\n",
        "                word_length = len(modified_word)\n",
        "                modified_index = random.randint(0, word_length - 1)\n",
        "                tokens[i] =  ''.join(modified_word[:modified_index ] + modified_word[modified_index + 1:])\n",
        "                out.append(1)\n",
        "                # thay 1 char bằng 1 char khác\n",
        "            elif prob < 0.8:\n",
        "                modified_word = list(token)\n",
        "                word_length = len(modified_word)\n",
        "                modified_index = random.randint(0, word_length - 1)\n",
        "                modified_word[modified_index] = random.choice(list2)\n",
        "                tokens[i] = ''.join(modified_word)\n",
        "                out.append(1)\n",
        "                #chuyển word về dạng telex của nó, nếu nó ko có từ nào chuyển được thì giữ nguyên\n",
        "            else:\n",
        "                vn_letters = [char for char in token if char in dict1.keys()]\n",
        "                if vn_letters:\n",
        "                    # Randomly select one of the Vietnamese letters\n",
        "                    modified_letter = random.choice(vn_letters)\n",
        "                    # Replace the letter with its corresponding Telex coding\n",
        "                    tokens[i] = tokens[i].replace(modified_letter, dict1[modified_letter])\n",
        "                    out.append(1)\n",
        "                else:\n",
        "                    out.append(0)\n",
        "        else:\n",
        "            out.append(0)\n",
        "        if count >= len(tokens) // 2:\n",
        "            out.extend([0] * (len(tokens) - count))\n",
        "            return ' '.join(tokens), out\n",
        "    return ' '.join(tokens),out\n",
        "def load_dataset(file_path):\n",
        "    dataset = []\n",
        "    with open(file_path, 'r',encoding= \"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            dataset.append(line)\n",
        "    return dataset\n",
        "#check chạy ok\n",
        "if __name__ == '_main_':\n",
        "    dataset = load_dataset(\"text2.txt\")\n",
        "    error_dataset= [create_Error(i, 0.2) for i in dataset]\n",
        "    df = pd.DataFrame(columns=['original','error','label'])\n",
        "    df['original'] = dataset\n",
        "    df['error'] = [x[0] for x in error_dataset]\n",
        "    df['label'] = [x[1] for x in error_dataset]\n",
        "    df = pd.DataFrame(columns=['original','error','label'], data=df)\n",
        "    df.to_csv('process_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "3IfM-crMIKi9",
        "outputId": "07e05e37-270b-492f-9bf2-c36248f1e1e8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original</th>\n",
              "      <th>error</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bình rất vui tính.</td>\n",
              "      <td>Bì5h ất vui tính.</td>\n",
              "      <td>[1, 1, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Chuẩn là ngừoi thông minh.</td>\n",
              "      <td>Chuẩ4 law ngừoi thông minh.</td>\n",
              "      <td>[1, 1, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     original                        error            label\n",
              "0          Bình rất vui tính.            Bì5h ất vui tính.     [1, 1, 0, 0]\n",
              "1  Chuẩn là ngừoi thông minh.  Chuẩ4 law ngừoi thông minh.  [1, 1, 0, 0, 0]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = [\"Bình rất vui tính.\", \"Chuẩn là ngừoi thông minh.\"]\n",
        "error_dataset= [create_Error(i, 1.1) for i in dataset]\n",
        "df = pd.DataFrame(columns=['original','error','label'])\n",
        "df['original'] = dataset\n",
        "df['error'] = [x[0] for x in error_dataset]\n",
        "df['label'] = [x[1] for x in error_dataset]\n",
        "df = pd.DataFrame(columns=['original','error','label'], data=df)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbHLf8yxvn6Z",
        "outputId": "fd39820c-8023-48e3-9b56-e229f45af6a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading file: Thế giới.txt\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/home/lenovo/Documents/1. FPT University/4.Project/ProcessedData/sent/Thế giới.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m mask_dataset\u001b[39m.\u001b[39mappend(mask)\n\u001b[1;32m     41\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 42\u001b[0m     append_sent_to_file(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(save_path_sent, os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mbasename(file_name)), error_dataset)\n\u001b[1;32m     43\u001b[0m     append_mask_to_file(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(save_path_mask, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(file_name)), mask_dataset)\n\u001b[1;32m     44\u001b[0m     \u001b[39m# break\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[3], line 16\u001b[0m, in \u001b[0;36mappend_sent_to_file\u001b[0;34m(file_path, dataset)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mappend_sent_to_file\u001b[39m(file_path, dataset):\n\u001b[0;32m---> 16\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(file_path, \u001b[39m'\u001b[39;49m\u001b[39ma\u001b[39;49m\u001b[39m'\u001b[39;49m,encoding \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m     17\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m dataset:\n\u001b[1;32m     18\u001b[0m             file\u001b[39m.\u001b[39mwrite(line \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/lenovo/Documents/1. FPT University/4.Project/ProcessedData/sent/Thế giới.txt'"
          ]
        }
      ],
      "source": [
        "import resource\n",
        "import os\n",
        "def save_sent(dataset, fileName):\n",
        "    with open(fileName, 'w',encoding = 'utf-8') as f:\n",
        "        for line in dataset:\n",
        "            f.write(line+'\\n')\n",
        "    print('Done')\n",
        "\n",
        "def save_mask(dataset, fileName):\n",
        "    with open(fileName, 'w',encoding = 'utf-8') as f:\n",
        "        for line in dataset:\n",
        "            f.write(' '.join(str(e) for e in line)+'\\n')\n",
        "    print('Done')\n",
        "\n",
        "def append_sent_to_file(file_path, dataset):\n",
        "    with open(file_path, 'a',encoding = 'utf-8') as file:\n",
        "        for line in dataset:\n",
        "            file.write(line + '\\n')\n",
        "\n",
        "def append_mask_to_file(file_path, dataset):\n",
        "    with open(file_path, 'a',encoding = 'utf-8') as file:\n",
        "        for mask in dataset:\n",
        "            file.write(' '.join(str(e) for e in mask)+'\\n')\n",
        "\n",
        "path = \"/home/lenovo/Documents/1. FPT University/4.Project/Data/news-corpus-categorys-20181217/corpus/\"\n",
        "save_path_sent = \"/home/lenovo/Documents/1. FPT University/4.Project/ProcessedData/sent/\"\n",
        "save_path_mask = \"/home/lenovo/Documents/1. FPT University/4.Project/ProcessedData/mask/\"\n",
        "for file_name in os.listdir(path):\n",
        "    print(\"Loading file:\", file_name)\n",
        "    file_name = os.path.join(path, file_name)\n",
        "    error_dataset = []\n",
        "    mask_dataset = []\n",
        "    i = 0\n",
        "    with open(file_name, encoding = 'utf-8') as txt_file:\n",
        "        for line in txt_file:\n",
        "            i = i + 1\n",
        "            error_sent,mask = create_Error(line,0.2)\n",
        "            error_dataset.append(error_sent)\n",
        "            mask_dataset.append(mask)\n",
        "            \n",
        "            if i % 100 == 0:\n",
        "                append_sent_to_file(os.path.join(save_path_sent, os.path.basename(file_name)), error_dataset)\n",
        "                append_mask_to_file(os.path.join(save_path_mask, os.path.basename(file_name)), mask_dataset)\n",
        "                # break\n",
        "                error_dataset = []\n",
        "                mask_dataset = []\n",
        "\n",
        "        # check if there are any sent in the last lines\n",
        "        if len(error_dataset) > 0:\n",
        "            append_sent_to_file(os.path.join(save_path_sent, os.path.basename(file_name)), error_dataset)\n",
        "            append_mask_to_file(os.path.join(save_path_mask, os.path.basename(file_name)), mask_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "append_sent_to_file(\"/home/lenovo/Documents/1. FPT University/4.Project/ProcessedData/Công nghệ sent.txt\", error_dataset)\n",
        "append_mask_to_file(\"/home/lenovo/Documents/1. FPT University/4.Project/ProcessedData/Công nghệ mask.txt\", mask_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'VeriME - hệ `inh tháu Xác minh Nhận dạng số dựa trên nền tảng Blockchain vừa chính thức kys kết 6ợp tác với Avvanz - ông ty công nghệ Quản lý vòng ờđi nhân viên với giải pháp tìm kieesm n8ân sự cho ftổ chức doanh nghiệp, trong việc cung cấp dịch vụ Xác minh nhận dạng số (D-KYC).'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "error_dataset[26]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
